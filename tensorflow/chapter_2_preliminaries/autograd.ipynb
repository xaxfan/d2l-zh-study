{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "836fdfbb",
   "metadata": {
    "origin_pos": 0
   },
   "source": [
    "# 自动微分\n",
    ":label:`sec_autograd`\n",
    "\n",
    "正如 :numref:`sec_calculus`中所说，求导是几乎所有深度学习优化算法的关键步骤。\n",
    "虽然求导的计算很简单，只需要一些基本的微积分。\n",
    "但对于复杂的模型，手工进行更新是一件很痛苦的事情（而且经常容易出错）。\n",
    "\n",
    "深度学习框架通过自动计算导数，即*自动微分*（automatic differentiation）来加快求导。\n",
    "实际中，根据设计好的模型，系统会构建一个*计算图*（computational graph），\n",
    "来跟踪计算是哪些数据通过哪些操作组合起来产生输出。\n",
    "自动微分使系统能够随后反向传播梯度。\n",
    "这里，*反向传播*（backpropagate）意味着跟踪整个计算图，填充关于每个参数的偏导数。\n",
    "\n",
    "## 一个简单的例子\n",
    "\n",
    "作为一个演示例子，(**假设我们想对函数$y=2\\mathbf{x}^{\\top}\\mathbf{x}$关于列向量$\\mathbf{x}$求导**)。\n",
    "首先，我们创建变量`x`并为其分配一个初始值。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd8ddd3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "`tensorflow.python.framework.ops.EagerTensor` 和 `tensorflow.python.ops.resource_variable_ops.ResourceVariable` 都是\n",
    "TensorFlow 中的数据类型，但它们有着不同的用途。\n",
    "\n",
    "`EagerTensor` 是 TensorFlow 中的一种张量类型，用于表示多维数组，它是 TensorFlow Eager Execution 模式的基础。\n",
    "Eager Execution 是一种命令式编程环境，可以让 TensorFlow 用户在 Python 中即时执行操作，而不需要构建计算图。\n",
    "EagerTensor 是在执行操作时创建的，它可以像普通的 Python 对象一样进行计算、赋值和索引等操作，这使得 TensorFlow 更加易于使用和调试。\n",
    "\n",
    "`ResourceVariable` 是 TensorFlow 中的一种变量类型，用于表示在模型训练过程中需要进行优化的可训练参数。\n",
    "ResourceVariable 是在计算图构建时创建的，它可以被赋值和更新，而这些操作都会被加入计算图中，从而使得 TensorFlow \n",
    "可以自动计算梯度并进行优化。与 EagerTensor 不同，ResourceVariable 是不可变的，它的值只能通过赋值或者更新操作来改变。\n",
    "\n",
    "总之，EagerTensor 用于表示张量，支持在 Eager Execution 模式下进行即时计算；而 ResourceVariable \n",
    "则用于表示模型中需要优化的参数，支持在计算图中进行自动求导和优化。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "33064f2f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-07T18:59:42.401367Z",
     "iopub.status.busy": "2022-12-07T18:59:42.400636Z",
     "iopub.status.idle": "2022-12-07T18:59:44.941832Z",
     "shell.execute_reply": "2022-12-07T18:59:44.940709Z"
    },
    "origin_pos": 3,
    "tab": [
     "tensorflow"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(4,), dtype=float32, numpy=array([0., 1., 2., 3.], dtype=float32)>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "x = tf.range(4, dtype=tf.float32)\n",
    "x\n",
    "# tensorflow.python.framework.ops.EagerTensor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef12d3fb",
   "metadata": {
    "origin_pos": 5
   },
   "source": [
    "[**在我们计算$y$关于$\\mathbf{x}$的梯度之前，需要一个地方来存储梯度。**]\n",
    "重要的是，我们不会在每次对一个参数求导时都分配新的内存。\n",
    "因为我们经常会成千上万次地更新相同的参数，每次都分配新的内存可能很快就会将内存耗尽。\n",
    "注意，一个标量函数关于向量$\\mathbf{x}$的梯度是向量，并且与$\\mathbf{x}$具有相同的形状。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "19429525",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-07T18:59:44.945659Z",
     "iopub.status.busy": "2022-12-07T18:59:44.944895Z",
     "iopub.status.idle": "2022-12-07T18:59:44.950728Z",
     "shell.execute_reply": "2022-12-07T18:59:44.949628Z"
    },
    "origin_pos": 8,
    "tab": [
     "tensorflow"
    ]
   },
   "outputs": [],
   "source": [
    "x = tf.Variable(x)\n",
    "# tensorflow.python.ops.resource_variable_ops.ResourceVariable"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50e20795",
   "metadata": {
    "origin_pos": 10
   },
   "source": [
    "(**现在计算$y$。**)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4c5cef30",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-07T18:59:44.954083Z",
     "iopub.status.busy": "2022-12-07T18:59:44.953585Z",
     "iopub.status.idle": "2022-12-07T18:59:44.963765Z",
     "shell.execute_reply": "2022-12-07T18:59:44.962721Z"
    },
    "origin_pos": 13,
    "tab": [
     "tensorflow"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(), dtype=float32, numpy=28.0>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 把所有计算记录在磁带上\n",
    "# \"with tf.GradientTape() as t:\" 这行代码使用 TensorFlow 的 GradientTape() 上下文管理器来记录计算图中的操作，以便于计算梯度。\n",
    "# 在上下文管理器内，可以通过调用 `t.watch()` 方法来追踪某个张量的梯度信息，也可以直接使用张量进行计算。\n",
    "# 完成计算后，可以通过 `t.gradient()` 方法计算被追踪张量的梯度。这个上下文管理器常用于训练神经网络中，用于自动计算梯度并更新参数。\n",
    "# y = 2* x^2\n",
    "\n",
    "with tf.GradientTape() as t:\n",
    "    y = 2 * tf.tensordot(x, x, axes=1)\n",
    "y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "432c5377",
   "metadata": {
    "origin_pos": 15
   },
   "source": [
    "`x`是一个长度为4的向量，计算`x`和`x`的点积，得到了我们赋值给`y`的标量输出。\n",
    "接下来，[**通过调用反向传播函数来自动计算`y`关于`x`每个分量的梯度**]，并打印这些梯度。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6760afa9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-07T18:59:44.967151Z",
     "iopub.status.busy": "2022-12-07T18:59:44.966413Z",
     "iopub.status.idle": "2022-12-07T18:59:44.976202Z",
     "shell.execute_reply": "2022-12-07T18:59:44.975124Z"
    },
    "origin_pos": 18,
    "tab": [
     "tensorflow"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(4,), dtype=float32, numpy=array([ 0.,  4.,  8., 12.], dtype=float32)>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_grad = t.gradient(y, x)  # = 4x\n",
    "x_grad"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2e01125",
   "metadata": {
    "origin_pos": 20
   },
   "source": [
    "函数$y=2\\mathbf{x}^{\\top}\\mathbf{x}$关于$\\mathbf{x}$的梯度应为$4\\mathbf{x}$。\n",
    "让我们快速验证这个梯度是否计算正确。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d0589678",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-07T18:59:44.979744Z",
     "iopub.status.busy": "2022-12-07T18:59:44.979017Z",
     "iopub.status.idle": "2022-12-07T18:59:44.985865Z",
     "shell.execute_reply": "2022-12-07T18:59:44.984783Z"
    },
    "origin_pos": 23,
    "tab": [
     "tensorflow"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(4,), dtype=bool, numpy=array([ True,  True,  True,  True])>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_grad == 4 * x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cdff342",
   "metadata": {
    "origin_pos": 25
   },
   "source": [
    "[**现在计算`x`的另一个函数。**]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ab5223fc",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-07T18:59:44.989308Z",
     "iopub.status.busy": "2022-12-07T18:59:44.988581Z",
     "iopub.status.idle": "2022-12-07T18:59:44.996466Z",
     "shell.execute_reply": "2022-12-07T18:59:44.995503Z"
    },
    "origin_pos": 28,
    "tab": [
     "tensorflow"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(4,), dtype=float32, numpy=array([1., 1., 1., 1.], dtype=float32)>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with tf.GradientTape() as t:\n",
    "    y = tf.reduce_sum(x)\n",
    "t.gradient(y, x)  # 被新计算的梯度覆盖 == 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a6a844e",
   "metadata": {
    "origin_pos": 30
   },
   "source": [
    "## 非标量变量的反向传播\n",
    "\n",
    "当`y`不是标量时，向量`y`关于向量`x`的导数的最自然解释是一个矩阵。\n",
    "对于高阶和高维的`y`和`x`，求导的结果可以是一个高阶张量。\n",
    "\n",
    "然而，虽然这些更奇特的对象确实出现在高级机器学习中（包括[**深度学习中**]），\n",
    "但当调用向量的反向计算时，我们通常会试图计算一批训练样本中每个组成部分的损失函数的导数。\n",
    "这里(**，我们的目的不是计算微分矩阵，而是单独计算批量中每个样本的偏导数之和。**)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4e031166",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-07T18:59:45.000039Z",
     "iopub.status.busy": "2022-12-07T18:59:44.999278Z",
     "iopub.status.idle": "2022-12-07T18:59:45.007215Z",
     "shell.execute_reply": "2022-12-07T18:59:45.006094Z"
    },
    "origin_pos": 33,
    "tab": [
     "tensorflow"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(4,), dtype=float32, numpy=array([0., 2., 4., 6.], dtype=float32)>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with tf.GradientTape() as t:\n",
    "    y = x * x\n",
    "t.gradient(y, x)  # 等价于y=tf.reduce_sum(x*x)\n",
    "\n",
    "# 是的，y=tf.reduce_sum(x*x)和y=x^2在x处的梯度是相同的。\n",
    "\n",
    "# 假设x是一个实数或一个向量，那么y=x^2的导数是2x，而y=tf.reduce_sum(x*x)的导数是2x的向量和，也就是说，每个x的元素都乘以2，\n",
    "# 然后所有结果相加。这意味着两个函数的梯度在每个元素处都相同，因此它们具有相同的梯度。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91f20705",
   "metadata": {
    "origin_pos": 35
   },
   "source": [
    "## 分离计算\n",
    "\n",
    "有时，我们希望[**将某些计算移动到记录的计算图之外**]。\n",
    "例如，假设`y`是作为`x`的函数计算的，而`z`则是作为`y`和`x`的函数计算的。\n",
    "想象一下，我们想计算`z`关于`x`的梯度，但由于某种原因，希望将`y`视为一个常数，\n",
    "并且只考虑到`x`在`y`被计算后发挥的作用。\n",
    "\n",
    "这里可以分离`y`来返回一个新变量`u`，该变量与`y`具有相同的值，\n",
    "但丢弃计算图中如何计算`y`的任何信息。\n",
    "换句话说，梯度不会向后流经`u`到`x`。\n",
    "因此，下面的反向传播函数计算`z=u*x`关于`x`的偏导数，同时将`u`作为常数处理，\n",
    "而不是`z=x*x*x`关于`x`的偏导数。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ae25f5de",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-07T18:59:45.010709Z",
     "iopub.status.busy": "2022-12-07T18:59:45.010053Z",
     "iopub.status.idle": "2022-12-07T18:59:45.018389Z",
     "shell.execute_reply": "2022-12-07T18:59:45.017420Z"
    },
    "origin_pos": 38,
    "tab": [
     "tensorflow"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(4,), dtype=bool, numpy=array([ True,  True,  True,  True])>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 设置persistent=True来运行t.gradient多次\n",
    "# 在 TensorFlow 中，tf.GradientTape() 可以用于记录计算梯度的操作。\n",
    "# 当 persistent=True 时，可以在一个 GradientTape 上多次调用 gradient() 方法，而不用担心中间结果被丢弃的问题。\n",
    "# 具体来说，当我们在一个 GradientTape 上调用 gradient() 方法后，这个 GradientTape 对象就被释放了，所有相关的资源也被回收。\n",
    "# 但是当 persistent=True 时，这个 GradientTape 对象不会被立即释放，而是等到显式调用 GradientTape.reset() 方法后才会被释放，\n",
    "# 这样就可以在同一个 GradientTape 上多次计算梯度了。\n",
    "with tf.GradientTape(persistent=True) as t:\n",
    "    y = x * x\n",
    "    u = tf.stop_gradient(y)\n",
    "    z = u * x\n",
    "\n",
    "x_grad = t.gradient(z, x)\n",
    "x_grad == u"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acbf5538",
   "metadata": {
    "origin_pos": 40
   },
   "source": [
    "由于记录了`y`的计算结果，我们可以随后在`y`上调用反向传播，\n",
    "得到`y=x*x`关于的`x`的导数，即`2*x`。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ed0369f4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-07T18:59:45.021472Z",
     "iopub.status.busy": "2022-12-07T18:59:45.020968Z",
     "iopub.status.idle": "2022-12-07T18:59:45.028273Z",
     "shell.execute_reply": "2022-12-07T18:59:45.027187Z"
    },
    "origin_pos": 43,
    "tab": [
     "tensorflow"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(4,), dtype=bool, numpy=array([ True,  True,  True,  True])>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t.gradient(y, x) == 2 * x \n",
    "# 如果没有设置“persistent=True”，这个执行失败，因为在上面x_grad = t.gradient(z, x)中，资源就被回收了"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d655919",
   "metadata": {
    "origin_pos": 45
   },
   "source": [
    "## Python控制流的梯度计算\n",
    "\n",
    "使用自动微分的一个好处是：\n",
    "[**即使构建函数的计算图需要通过Python控制流（例如，条件、循环或任意函数调用），我们仍然可以计算得到的变量的梯度**]。\n",
    "在下面的代码中，`while`循环的迭代次数和`if`语句的结果都取决于输入`a`的值。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1d6270ca",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-07T18:59:45.031616Z",
     "iopub.status.busy": "2022-12-07T18:59:45.031117Z",
     "iopub.status.idle": "2022-12-07T18:59:45.036378Z",
     "shell.execute_reply": "2022-12-07T18:59:45.035299Z"
    },
    "origin_pos": 48,
    "tab": [
     "tensorflow"
    ]
   },
   "outputs": [],
   "source": [
    "def f(a):\n",
    "    b = a * 2\n",
    "    while tf.norm(b) < 1000:\n",
    "        b = b * 2\n",
    "    if tf.reduce_sum(b) > 0:\n",
    "        c = b\n",
    "    else:\n",
    "        c = 100 * b\n",
    "    return c"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91c1937f",
   "metadata": {
    "origin_pos": 50
   },
   "source": [
    "让我们计算梯度。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ec73b5fa",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-07T18:59:45.039664Z",
     "iopub.status.busy": "2022-12-07T18:59:45.039166Z",
     "iopub.status.idle": "2022-12-07T18:59:45.056037Z",
     "shell.execute_reply": "2022-12-07T18:59:45.054936Z"
    },
    "origin_pos": 53,
    "tab": [
     "tensorflow"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(), dtype=float32, numpy=32768.0>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = tf.Variable(tf.random.normal(shape=()))   #生成一个随机数\n",
    "with tf.GradientTape() as t:\n",
    "    d = f(a)\n",
    "d_grad = t.gradient(d, a)\n",
    "d_grad"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06b7188e",
   "metadata": {
    "origin_pos": 55
   },
   "source": [
    "我们现在可以分析上面定义的`f`函数。\n",
    "请注意，它在其输入`a`中是分段线性的。\n",
    "换言之，对于任何`a`，存在某个常量标量`k`，使得`f(a)=k*a`，其中`k`的值取决于输入`a`，因此可以用`d/a`验证梯度是否正确。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6a2aeae3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-07T18:59:45.059557Z",
     "iopub.status.busy": "2022-12-07T18:59:45.058742Z",
     "iopub.status.idle": "2022-12-07T18:59:45.065650Z",
     "shell.execute_reply": "2022-12-07T18:59:45.064561Z"
    },
    "origin_pos": 58,
    "tab": [
     "tensorflow"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(), dtype=bool, numpy=True>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d_grad == d / a"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05fdcfa7",
   "metadata": {
    "origin_pos": 60
   },
   "source": [
    "## 小结\n",
    "\n",
    "* 深度学习框架可以自动计算导数：我们首先将梯度附加到想要对其计算偏导数的变量上，然后记录目标值的计算，执行它的反向传播函数，并访问得到的梯度。\n",
    "\n",
    "## 练习\n",
    "\n",
    "1. 为什么计算二阶导数比一阶导数的开销要更大？\n",
    "1. 在运行反向传播函数之后，立即再次运行它，看看会发生什么。\n",
    "1. 在控制流的例子中，我们计算`d`关于`a`的导数，如果将变量`a`更改为随机向量或矩阵，会发生什么？\n",
    "1. 重新设计一个求控制流梯度的例子，运行并分析结果。\n",
    "1. 使$f(x)=\\sin(x)$，绘制$f(x)$和$\\frac{df(x)}{dx}$的图像，其中后者不使用$f'(x)=\\cos(x)$。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96cadce4",
   "metadata": {
    "origin_pos": 63,
    "tab": [
     "tensorflow"
    ]
   },
   "source": [
    "[Discussions](https://discuss.d2l.ai/t/1757)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
